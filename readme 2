# Deterministic Reinforcement Learning — Value Iteration, Policy Iteration, and Q-Learning

## 1. Assignment Context

This repository contains my implementation of three classical reinforcement learning algorithms:

- **Value Iteration**
- **Policy Iteration**
- **Q-Learning**

applied to the deterministic gridworld defined in the assignment:

- States:  
  \{(0,0), (1,0), (2,0), (0,1), (1,1), (2,1), (0,2), (1,2), (2,2), (0,3), (1,3), (2,3)\}
- Terminal states:  
  \{(1,1), (1,2), (2,1), (2,3)\}
- Rewards:  
  - (1,1): −10  
  - (2,1): −20  
  - (1,2): +10  
  - (2,3): +20  
  - All other states: 0  

The agent can move **Up (U), Down (D), Left (L), Right (R)**.  
Actions are deterministic; attempts to move outside the grid keep the agent in the same state.  

The assignment requires comparing algorithm behavior under three discount factors:

- γ = 0.9  
- γ = 0.5  
- γ = 0.1  

and analyzing:

- Which algorithm converges fastest  
- How γ affects:
  - value function magnitude  
  - long-term vs short-term preference  
  - policy behavior  


## 2. Files and How to Run

**File:** `deterministic_rl.py`

This single script includes:

- Environment definition
- Value Iteration implementation
- Policy Iteration implementation
- Q-Learning implementation
- Utility functions for printing value grids, policy grids, and Q summaries
- A `run_all()` function that runs all experiments for γ ∈ {0.9, 0.5, 0.1}
- A simple main block:

```python
if __name__ == "__main__":
    run_all()
